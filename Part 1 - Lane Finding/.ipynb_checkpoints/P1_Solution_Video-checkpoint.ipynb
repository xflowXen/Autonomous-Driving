{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Project 1 - Finding Lanes on the road - Video Output\n",
    "\n",
    "The below is the final version of the code used to process the video images to find lane lines. In the real world - this pipeline could be used to capture live video data and process in real time, while relaying information back to an automotive control system.\n",
    "\n",
    "In its current form though, there are a number of additional optimisations which could be made to enhance accuracy and speed:\n",
    "* Cross-frame smoothing - for times where the lines detected change quickly between frames but only for a small number of frames\n",
    "* Additional colour pallettes - to avoid detection of shadows on the side of the road as lines and to deal with shadows obscuring the lane line\n",
    "* FPGA deployment - to speed up the processing times - currently its not able to process in real-time.\n",
    "\n",
    "The videos can be viewed by navigating to the test_videos_output directory [here](test_videos_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in int_scalars\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "def processFrame(frame):\n",
    "    '''processes the frame and highlights the lane lines'''\n",
    "    #Define area of interest points as a function of frame size\n",
    "    region_point_1 = [frame.shape[1]*0.05,frame.shape[0]] \n",
    "    region_point_2 = [frame.shape[1]*0.95,frame.shape[0]]\n",
    "    region_point_3 = [frame.shape[1]/2,frame.shape[0]/2]\n",
    "    line_A = np.polyfit((region_point_3[0], region_point_1[0]), (region_point_3[1], region_point_1[1]), 1)\n",
    "    line_B = np.polyfit((region_point_3[0], region_point_2[0]), (region_point_3[1], region_point_2[1]), 1)\n",
    "    line_C = np.polyfit((region_point_1[0], region_point_2[0]), (region_point_1[1], region_point_2[1]), 1)\n",
    "    \n",
    "    #Discard all data not in the area of interest\n",
    "    frame_51 = np.copy(frame)\n",
    "    (y,x) = (frame_51.shape[0],frame_51.shape[1])\n",
    "    (xx, yy) = np.meshgrid(np.arange(0,x),np.arange(0,y))\n",
    "    area_of_interest = (yy > (xx*line_A[0]+line_A[1])) & (yy > (xx*line_B[0] + line_B[1])) & (yy < (xx*line_C[0] + line_C[1]))\n",
    "    discarded_area = np.invert(area_of_interest)\n",
    "    frame_51[discarded_area] = [0,0,0]\n",
    "    \n",
    "    #Apply image transform and edge detection\n",
    "    low_threshold = 254\n",
    "    high_threshold = 255\n",
    "    frame_51 = cv2.GaussianBlur(frame_51,(3, 3),0)\n",
    "    frame_51 = cv2.cvtColor(frame_51,cv2.COLOR_RGB2GRAY)\n",
    "    frame_51 = cv2.Canny(frame_51, low_threshold, high_threshold)\n",
    "    \n",
    "    #Remove area of interest lines from edge detection\n",
    "    region_point_1 = [frame.shape[1]*0.05 + 1,frame.shape[0]] \n",
    "    region_point_2 = [frame.shape[1]*0.95 - 1,frame.shape[0]]\n",
    "    region_point_3 = [frame.shape[1]/2,(frame.shape[0]/2)+1]\n",
    "    line_A = np.polyfit((region_point_3[0], region_point_1[0]), (region_point_3[1], region_point_1[1]), 1)\n",
    "    line_B = np.polyfit((region_point_3[0], region_point_2[0]), (region_point_3[1], region_point_2[1]), 1)\n",
    "    line_C = np.polyfit((region_point_1[0], region_point_2[0]), (region_point_1[1], region_point_2[1]), 1)\n",
    "    (y,x) = (frame.shape[0],frame.shape[1])\n",
    "    (xx, yy) = np.meshgrid(np.arange(0,x),np.arange(0,y))\n",
    "    reduced_aoi = (yy > (xx*line_A[0]+line_A[1])) & (yy > (xx*line_B[0] + line_B[1])) & (yy < (xx*line_C[0] + line_C[1]))\n",
    "    discard_area = np.invert(reduced_aoi)\n",
    "    frame_51[discard_area] = 0\n",
    "    \n",
    "    # Define Hough parameters\n",
    "    rho = 1\n",
    "    theta = np.pi/180\n",
    "    threshold = 1\n",
    "    min_line_length = 10\n",
    "    max_line_gap = 1\n",
    "    line_image = np.copy(frame_51)*0 #creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    lines = cv2.HoughLinesP(frame_51, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "    \n",
    "    #Now use detected Hough lines to determine how to generate the lane lines\n",
    "    intercept_threshold = 100\n",
    "    linegroups = {}\n",
    "    linegroups_lengths = {}\n",
    "    linegroups_avg_max_intercept = {}\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            #Using c = y - mx to determine the y-intercept of this line\n",
    "            m = (y2-y1)/(x2-x1)\n",
    "            c = y1 - (m*x1)\n",
    "            #Was going to use the length to determine which lines take priority but the intercept threshold\n",
    "            #seems to do a good enough job.\n",
    "            #length = np.sqrt((x2-x1)**2 + (y2-y1)**2) \n",
    "\n",
    "            #Now substituting, we can find the max-intercept (i.e where y=max using the form xi = (y-c)/m )\n",
    "            #since (0,0) starts in the top left. However we only want to get this when the gradient is non-zero. \n",
    "            if (m > 0 or m < 0) and m != math.inf and m != -math.inf:\n",
    "                max_intercept = (frame_51.shape[0] - c)/m #using the shape instead of 0 since the graph is inverted\n",
    "                if len(linegroups) == 0:\n",
    "                    linegroups[round(max_intercept)] = []\n",
    "                    linegroups[round(max_intercept)].append(line)\n",
    "                    linegroups_avg_max_intercept[round(max_intercept)] = []\n",
    "                    linegroups_avg_max_intercept[round(max_intercept)].append(max_intercept)\n",
    "                    #linegroups_lengths[round(max_intercept)] = []\n",
    "                    #linegroups_lengths[round(max_intercept)].append(length)\n",
    "                else:\n",
    "                    groupFound = False\n",
    "                    for key in linegroups.keys():\n",
    "                        if max_intercept >= key-intercept_threshold and max_intercept <= key + intercept_threshold:\n",
    "                            linegroups[key].append(line)\n",
    "                            linegroups_avg_max_intercept[key].append(max_intercept)\n",
    "                            #linegroups_lengths[key].append(length)\n",
    "                            groupFound = True\n",
    "                    if groupFound == False:\n",
    "                        linegroups[round(max_intercept)] = []\n",
    "                        linegroups[round(max_intercept)].append(line)\n",
    "                        linegroups_avg_max_intercept[round(max_intercept)] = []\n",
    "                        linegroups_avg_max_intercept[round(max_intercept)].append(max_intercept)\n",
    "                        #linegroups_lengths[round(x_intercept)] = []\n",
    "                        #linegroups_lengths[round(x_intercept)].append(length)\n",
    "\n",
    "    #Now we have our line groups, select the two that have the most lines clustered\n",
    "    max_values = [[0,0],[0,0]]\n",
    "    for key in linegroups.keys():\n",
    "        if len(linegroups[key]) > max_values[1][1]:\n",
    "            max_values[1] = [key, len(linegroups[key])]\n",
    "        if max_values[0][1] < max_values[1][1]:\n",
    "            temp = max_values[0]\n",
    "            max_values[0] = max_values[1]\n",
    "            max_values[1] = temp\n",
    "\n",
    "    if not (max_values[0] == [0,0] or max_values[1] == [0,0]): #skips frame if we can't find two lanes\n",
    "        #These two are most likely to be the lane lines. Now we can compute an average line from each of these\n",
    "        #groups and display over our lane lines\n",
    "        line1_x1 = []\n",
    "        line1_x2 = []\n",
    "        line1_y1 = []\n",
    "        line1_y2 = []\n",
    "        line2_x1 = []\n",
    "        line2_x2 = []\n",
    "        line2_y1 = []\n",
    "        line2_y2 = []\n",
    "\n",
    "        for line in linegroups[max_values[0][0]]:\n",
    "            line1_x1.append(line[0][0])\n",
    "            line1_y1.append(line[0][1])\n",
    "            line1_x2.append(line[0][2])\n",
    "            line1_y2.append(line[0][3])\n",
    "        min_l1_x = int(np.min(line1_x1 + line1_x2))\n",
    "        min_l1_y = int(np.min(line1_y1 + line1_y2))\n",
    "        max_l1_x = int(np.max(line1_x1 + line1_x2))\n",
    "        max_l1_y = int(np.max(line1_y1 + line1_y2))\n",
    "        #all_l1_m = np.divide(np.subtract(line1_y2, line1_y1), np.subtract(line1_x2, line1_x1))\n",
    "        average_l1_m = np.mean(np.divide(np.subtract(line1_y2, line1_y1), np.subtract(line1_x2, line1_x1)))\n",
    "        average_l1_max_intercept = int(np.mean(linegroups_avg_max_intercept[max_values[0][0]]))\n",
    "\n",
    "        #This is weird thinking in diverging flipped gradients\n",
    "        if average_l1_m > 0:  \n",
    "            l1_extrap_point = (min_l1_x, min_l1_y)\n",
    "        else:\n",
    "            l1_extrap_point = (max_l1_x, min_l1_y)\n",
    "\n",
    "\n",
    "        for line in linegroups[max_values[1][0]]:\n",
    "            line2_x1.append(line[0][0])\n",
    "            line2_y1.append(line[0][1])\n",
    "            line2_x2.append(line[0][2])\n",
    "            line2_y2.append(line[0][3])\n",
    "        min_l2_x = int(np.min(line2_x1 + line2_x2))\n",
    "        min_l2_y = int(np.min(line2_y1 + line2_y2))\n",
    "        max_l2_x = int(np.max(line2_x1 + line2_x2))\n",
    "        max_l2_y = int(np.max(line2_y1 + line2_y2))\n",
    "        average_l2_m = np.mean(np.divide(np.subtract(line2_y2, line2_y1), np.subtract(line2_x2, line2_x1)))\n",
    "        average_l2_max_intercept = int(np.mean(linegroups_avg_max_intercept[max_values[1][0]]))\n",
    "        if average_l2_m > 0:\n",
    "            l2_extrap_point = (min_l2_x, min_l2_y)\n",
    "        else:\n",
    "            l2_extrap_point = (max_l2_x, min_l2_y)\n",
    "\n",
    "\n",
    "        line_image = np.zeros_like(frame_51)\n",
    "        cv2.line(line_image,l1_extrap_point,(average_l1_max_intercept,frame_51.shape[0]), (255,0,0), 15)\n",
    "        cv2.line(line_image,l2_extrap_point,(average_l2_max_intercept,frame_51.shape[0]), (255,0,0), 15)\n",
    "\n",
    "\n",
    "        #And finally - extrapolate the lines to a point just below the apex of our area of interest \n",
    "        extrap_to_y = int(region_point_3[1] + (frame.shape[0]*0.1))\n",
    "\n",
    "        extrap_l1_m = (frame_51.shape[0]-l1_extrap_point[1])/(average_l1_max_intercept-l1_extrap_point[0])\n",
    "        #m = y2-y1/x2-x1 => x2-x1 = (y2-y1)/m => x1 = x2 -((y2-y1)/m)\n",
    "        extrap_l1_x = int(average_l1_max_intercept - ((frame_51.shape[0]-extrap_to_y)/extrap_l1_m))\n",
    "        l1_extrap_point = (extrap_l1_x,extrap_to_y)\n",
    "\n",
    "        extrap_l2_m = (frame_51.shape[0]-l2_extrap_point[1])/(average_l2_max_intercept-l2_extrap_point[0])\n",
    "        extrap_l2_x = int(average_l2_max_intercept - ((frame_51.shape[0]-extrap_to_y)/extrap_l2_m))\n",
    "        l2_extrap_point = (extrap_l2_x,extrap_to_y)\n",
    "\n",
    "        #And draw these lines on our original images\n",
    "        line_image = np.zeros_like(frame)\n",
    "        cv2.line(line_image,l1_extrap_point,(average_l1_max_intercept,frame.shape[0]), (255,0,0), 15)\n",
    "        cv2.line(line_image,l2_extrap_point,(average_l2_max_intercept,frame.shape[0]), (255,0,0), 15)\n",
    "        frame = cv2.addWeighted(frame, 1.0, line_image,0.95,0.0) \n",
    "      \n",
    "    return frame\n",
    "    \n",
    "#Main loop to process image data from video files\n",
    "video_path = os.getcwd() + '/test_videos/'\n",
    "video_path_iterator = os.listdir(video_path)\n",
    "video_path_output = os.getcwd() + '/test_videos_output/'\n",
    "\n",
    "\n",
    "for video_name in video_path_iterator:\n",
    "    frames = cv2.VideoCapture(video_path + video_name)\n",
    "    (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "    if int(major_ver)  < 3 :\n",
    "        fps = frames.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    else :\n",
    "        fps = frames.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    \n",
    "    \n",
    "    #Now begin processing\n",
    "    flag,frame = frames.read()\n",
    "    v_out = cv2.VideoWriter(video_path_output + video_name, fourcc, fps, (frame.shape[1],frame.shape[0]))\n",
    "    while flag:\n",
    "        v_out.write(processFrame(frame))\n",
    "        flag, frame = frames.read()\n",
    "    \n",
    "    frames.release()\n",
    "    v_out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
